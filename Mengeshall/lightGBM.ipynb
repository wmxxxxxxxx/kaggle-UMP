{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\dask\\dataframe\\utils.py:366: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  _numeric_index_types = (pd.Int64Index, pd.Float64Index, pd.UInt64Index)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\dask\\dataframe\\utils.py:366: FutureWarning: pandas.Float64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  _numeric_index_types = (pd.Int64Index, pd.Float64Index, pd.UInt64Index)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\dask\\dataframe\\utils.py:366: FutureWarning: pandas.UInt64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  _numeric_index_types = (pd.Int64Index, pd.Float64Index, pd.UInt64Index)\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from scipy.stats import pearsonr\n",
    "import joblib\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, GroupKFold\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# data = pd.read_csv(\"C:\\\\Users\\\\Mengxiao.Wu\\\\Desktop\\\\train\\\\train.csv\")\n",
    "# data.to_hdf(\"C:\\\\Users\\\\Mengxiao.Wu\\\\Desktop\\\\train\\\\train.h5\", \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.to_hdf(\"C:\\\\Users\\\\Mengxiao.Wu\\\\Desktop\\\\train\\\\train.h5\", \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3140461, 304)\n",
      "Wall time: 45.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data = pd.read_hdf(\"C:\\\\Users\\\\Mengxiao.Wu\\\\Desktop\\\\train\\\\winsorize.h5\", \"train\")\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data['investment_id'] = data['investment_id'].astype('category')\n",
    "feats = [col for col in data.columns if col not in ['time_id', 'row_id', 'target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # 参数\n",
    "# params = {\n",
    "#     'task': 'train',\n",
    "#     'boosting_type': 'gbdt',  # 设置提升类型\n",
    "#     'objective': 'regression',  # 目标函数\n",
    "#     'metric': {'auc'},  # 评估函数\n",
    "#     'max_bin': 255,  # 大会有更准的效果,更慢的速度\n",
    "#     'min_data_in_leaf': 91,\n",
    "#     'num_iterations': 500,\n",
    "#     'max_depth': 4,  # 指定树的最大深度 [3, 5, 6, 7, 9, 12, 15, 17, 25]\n",
    "#     'num_leaves': 10,  # 叶子节点数\n",
    "#     'learning_rate': 0.1,  # 学习速率 [0.01, 0.015, 0.025, 0.05, 0.1]\n",
    "#     'feature_fraction': 1.0,  # 建树的特征选择比例 [0.6, 0.7, 0.8, 0.9, 1]\n",
    "#     'bagging_fraction': 0.6,  # 建树的样本采样比例 [0.6, 0.7, 0.8, 0.9, 1]\n",
    "#     # 'bagging_freq': 5,  # k 意味着每 k 次迭代执行bagging\n",
    "#     'min_sum_hessian_in_leaf': 3.0,  # 防止过拟合\n",
    "#     'verbose': 1  # <0 显示致命的, =0 显示错误 (警告), >0 显示信息\n",
    "# }\n",
    "params = {\n",
    "        'learning_rate':0.1,\n",
    "        \"objective\": \"regression\",\n",
    "        \"metric\": \"rmse\",\n",
    "        'boosting_type': \"gbdt\",\n",
    "        'verbosity': -1,\n",
    "        'n_jobs': -1, \n",
    "        'seed': 21,\n",
    "        'lambda_l1': 1.1895057699067542, \n",
    "        'lambda_l2': 1.9079686837880768e-08, \n",
    "        'num_leaves': 112, \n",
    "        'subsample':None,\n",
    "        'feature_fraction': 0.6259927292757151, \n",
    "        'bagging_fraction': 0.9782210574588895, \n",
    "        'bagging_freq': 1, \n",
    "        'n_estimators': 306, \n",
    "        'max_depth': 12, \n",
    "        'max_bin': 255, \n",
    "        'min_data_in_leaf': 366,\n",
    "        'colsample_bytree': None,\n",
    "        'subsample_freq': None,\n",
    "        'min_child_samples': None,\n",
    "        'reg_lambda': None,\n",
    "        'reg_alpha': None,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_splits = 10\n",
    "kfold = GroupKFold(n_splits)\n",
    "fold_scores = []\n",
    "models = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's rmse: 0.912714\n",
      "Fold 1: 0.13155193404420915\n",
      "[100]\tvalid_0's rmse: 0.909715\n",
      "[200]\tvalid_0's rmse: 0.909467\n",
      "Fold 2: 0.13787007905067414\n",
      "[100]\tvalid_0's rmse: 0.906641\n",
      "[200]\tvalid_0's rmse: 0.906251\n",
      "Fold 3: 0.1503298697450862\n",
      "[100]\tvalid_0's rmse: 0.915343\n",
      "[200]\tvalid_0's rmse: 0.914865\n",
      "[300]\tvalid_0's rmse: 0.914713\n",
      "Fold 4: 0.14380118086217256\n",
      "[100]\tvalid_0's rmse: 0.908924\n",
      "[200]\tvalid_0's rmse: 0.908308\n",
      "[300]\tvalid_0's rmse: 0.908199\n",
      "Fold 5: 0.15249867329671551\n",
      "[100]\tvalid_0's rmse: 0.909472\n",
      "[200]\tvalid_0's rmse: 0.908865\n",
      "[300]\tvalid_0's rmse: 0.908688\n",
      "Fold 6: 0.14902497024662895\n",
      "[100]\tvalid_0's rmse: 0.908329\n",
      "[200]\tvalid_0's rmse: 0.907874\n",
      "Fold 7: 0.1507330766044804\n",
      "[100]\tvalid_0's rmse: 0.90758\n",
      "[200]\tvalid_0's rmse: 0.907123\n",
      "Fold 8: 0.15570067679368396\n",
      "[100]\tvalid_0's rmse: 0.910214\n",
      "Fold 9: 0.12809671929567543\n",
      "[100]\tvalid_0's rmse: 0.914056\n",
      "[200]\tvalid_0's rmse: 0.913209\n",
      "[300]\tvalid_0's rmse: 0.912871\n",
      "Fold 10: 0.16068707995530573\n",
      "Overall score: 0.1460294259894632\n",
      "Wall time: 26min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for fold, (trn_idx, val_idx) in enumerate(kfold.split(data[feats], data.target, groups=data.time_id)):\n",
    "    X_train, y_train = data[feats].iloc[trn_idx], data['target'].iloc[trn_idx]\n",
    "    X_val, y_val = data[feats].iloc[val_idx], data['target'].iloc[val_idx]\n",
    "\n",
    "    model = lgb.LGBMRegressor(**params)\n",
    "\n",
    "    model.fit(X_train, y_train, eval_metric='rmse', eval_set=[(X_val, y_val)], verbose=100, early_stopping_rounds=50)\n",
    "\n",
    "    joblib.dump(model, f'lgbm_fold_{fold}.pkl')\n",
    "    md = joblib.load(f'lgbm_fold_{fold}.pkl')\n",
    "    y_pred = md.predict(X_val)\n",
    "#     y_pred = model.predict(X_val)\n",
    "#     y_pred = []\n",
    "#     for model in models:\n",
    "#         y_pred.append(model.predict(X_val))\n",
    "\n",
    "#     y_pred = np.nanmean(y_pred, axis = 0)\n",
    "\n",
    "    score = pearsonr(y_pred, y_val)[0]\n",
    "    print(f\"Fold {fold + 1}: {score}\")\n",
    "\n",
    "    fold_scores.append(score)\n",
    "    models.append(model)\n",
    "\n",
    "    del model, y_pred, score, X_train, y_train, X_val, y_val\n",
    "    gc.collect()\n",
    "    \n",
    "print(f\"Overall score: {np.mean(fold_scores, axis=0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBMRegressor(bagging_fraction=0.9782210574588895, bagging_freq=1,\n",
      "              colsample_bytree=None, feature_fraction=0.6259927292757151,\n",
      "              lambda_l1=1.1895057699067542, lambda_l2=1.9079686837880768e-08,\n",
      "              max_bin=255, max_depth=12, metric='rmse', min_child_samples=None,\n",
      "              min_data_in_leaf=366, n_estimators=306, num_leaves=112,\n",
      "              objective='regression', reg_alpha=None, reg_lambda=None, seed=21,\n",
      "              subsample=None, subsample_freq=None, verbosity=-1)\n",
      "LGBMRegressor(bagging_fraction=0.9782210574588895, bagging_freq=1,\n",
      "              colsample_bytree=None, feature_fraction=0.6259927292757151,\n",
      "              lambda_l1=1.1895057699067542, lambda_l2=1.9079686837880768e-08,\n",
      "              max_bin=255, max_depth=12, metric='rmse', min_child_samples=None,\n",
      "              min_data_in_leaf=366, n_estimators=306, num_leaves=112,\n",
      "              objective='regression', reg_alpha=None, reg_lambda=None, seed=21,\n",
      "              subsample=None, subsample_freq=None, verbosity=-1)\n",
      "LGBMRegressor(bagging_fraction=0.9782210574588895, bagging_freq=1,\n",
      "              colsample_bytree=None, feature_fraction=0.6259927292757151,\n",
      "              lambda_l1=1.1895057699067542, lambda_l2=1.9079686837880768e-08,\n",
      "              max_bin=255, max_depth=12, metric='rmse', min_child_samples=None,\n",
      "              min_data_in_leaf=366, n_estimators=306, num_leaves=112,\n",
      "              objective='regression', reg_alpha=None, reg_lambda=None, seed=21,\n",
      "              subsample=None, subsample_freq=None, verbosity=-1)\n",
      "LGBMRegressor(bagging_fraction=0.9782210574588895, bagging_freq=1,\n",
      "              colsample_bytree=None, feature_fraction=0.6259927292757151,\n",
      "              lambda_l1=1.1895057699067542, lambda_l2=1.9079686837880768e-08,\n",
      "              max_bin=255, max_depth=12, metric='rmse', min_child_samples=None,\n",
      "              min_data_in_leaf=366, n_estimators=306, num_leaves=112,\n",
      "              objective='regression', reg_alpha=None, reg_lambda=None, seed=21,\n",
      "              subsample=None, subsample_freq=None, verbosity=-1)\n",
      "LGBMRegressor(bagging_fraction=0.9782210574588895, bagging_freq=1,\n",
      "              colsample_bytree=None, feature_fraction=0.6259927292757151,\n",
      "              lambda_l1=1.1895057699067542, lambda_l2=1.9079686837880768e-08,\n",
      "              max_bin=255, max_depth=12, metric='rmse', min_child_samples=None,\n",
      "              min_data_in_leaf=366, n_estimators=306, num_leaves=112,\n",
      "              objective='regression', reg_alpha=None, reg_lambda=None, seed=21,\n",
      "              subsample=None, subsample_freq=None, verbosity=-1)\n",
      "LGBMRegressor(bagging_fraction=0.9782210574588895, bagging_freq=1,\n",
      "              colsample_bytree=None, feature_fraction=0.6259927292757151,\n",
      "              lambda_l1=1.1895057699067542, lambda_l2=1.9079686837880768e-08,\n",
      "              max_bin=255, max_depth=12, metric='rmse', min_child_samples=None,\n",
      "              min_data_in_leaf=366, n_estimators=306, num_leaves=112,\n",
      "              objective='regression', reg_alpha=None, reg_lambda=None, seed=21,\n",
      "              subsample=None, subsample_freq=None, verbosity=-1)\n",
      "LGBMRegressor(bagging_fraction=0.9782210574588895, bagging_freq=1,\n",
      "              colsample_bytree=None, feature_fraction=0.6259927292757151,\n",
      "              lambda_l1=1.1895057699067542, lambda_l2=1.9079686837880768e-08,\n",
      "              max_bin=255, max_depth=12, metric='rmse', min_child_samples=None,\n",
      "              min_data_in_leaf=366, n_estimators=306, num_leaves=112,\n",
      "              objective='regression', reg_alpha=None, reg_lambda=None, seed=21,\n",
      "              subsample=None, subsample_freq=None, verbosity=-1)\n",
      "LGBMRegressor(bagging_fraction=0.9782210574588895, bagging_freq=1,\n",
      "              colsample_bytree=None, feature_fraction=0.6259927292757151,\n",
      "              lambda_l1=1.1895057699067542, lambda_l2=1.9079686837880768e-08,\n",
      "              max_bin=255, max_depth=12, metric='rmse', min_child_samples=None,\n",
      "              min_data_in_leaf=366, n_estimators=306, num_leaves=112,\n",
      "              objective='regression', reg_alpha=None, reg_lambda=None, seed=21,\n",
      "              subsample=None, subsample_freq=None, verbosity=-1)\n",
      "LGBMRegressor(bagging_fraction=0.9782210574588895, bagging_freq=1,\n",
      "              colsample_bytree=None, feature_fraction=0.6259927292757151,\n",
      "              lambda_l1=1.1895057699067542, lambda_l2=1.9079686837880768e-08,\n",
      "              max_bin=255, max_depth=12, metric='rmse', min_child_samples=None,\n",
      "              min_data_in_leaf=366, n_estimators=306, num_leaves=112,\n",
      "              objective='regression', reg_alpha=None, reg_lambda=None, seed=21,\n",
      "              subsample=None, subsample_freq=None, verbosity=-1)\n",
      "LGBMRegressor(bagging_fraction=0.9782210574588895, bagging_freq=1,\n",
      "              colsample_bytree=None, feature_fraction=0.6259927292757151,\n",
      "              lambda_l1=1.1895057699067542, lambda_l2=1.9079686837880768e-08,\n",
      "              max_bin=255, max_depth=12, metric='rmse', min_child_samples=None,\n",
      "              min_data_in_leaf=366, n_estimators=306, num_leaves=112,\n",
      "              objective='regression', reg_alpha=None, reg_lambda=None, seed=21,\n",
      "              subsample=None, subsample_freq=None, verbosity=-1)\n"
     ]
    }
   ],
   "source": [
    "pkl = []\n",
    "for i in range(10):\n",
    "    model_name = f'lgbm_fold_{i}.pkl'\n",
    "    with open(model_name, 'rb') as f:\n",
    "        p = joblib.load(f)\n",
    "        print(p)\n",
    "    pkl.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: 0.4180587634659933\n",
      "Fold 2: 0.40606839772575015\n",
      "Fold 3: 0.42077108826240767\n",
      "Fold 4: 0.40933076411135055\n",
      "Fold 5: 0.42032121217273893\n",
      "Fold 6: 0.41401735029141\n",
      "Fold 7: 0.42719775995834913\n",
      "Fold 8: 0.4201127575169891\n",
      "Fold 9: 0.4214549390588938\n",
      "Fold 10: 0.42678793270824766\n",
      "Overall score: 0.41841209652721306\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for fold, (trn_idx, val_idx) in enumerate(kfold.split(data[feats], data.target, groups=data.time_id)):\n",
    "    X_train, y_train = data[feats].iloc[trn_idx], data['target'].iloc[trn_idx]\n",
    "    X_val, y_val = data[feats].iloc[val_idx], data['target'].iloc[val_idx]\n",
    "    \n",
    "    y_pred = []\n",
    "    for p in pkl:\n",
    "        y_pred.append(p.predict(X_val))\n",
    "\n",
    "    y_pred = np.mean(y_pred, axis = 0)\n",
    "\n",
    "    score = pearsonr(y_pred, y_val)[0]\n",
    "    print(f\"Fold {fold + 1}: {score}\")\n",
    "\n",
    "    scores.append(score)\n",
    "\n",
    "    del y_pred, score, X_train, y_train, X_val, y_val\n",
    "    gc.collect()\n",
    "    \n",
    "print(f\"Overall score: {np.mean(scores, axis=0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "10\n",
      "Overall score: 0.4184383143456026\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "print(1)\n",
    "X, y = data[feats], data['target']\n",
    "y_pred = []\n",
    "for model in models:\n",
    "    y_pred.append(model.predict(X))\n",
    "    \n",
    "print(len(y_pred))\n",
    "\n",
    "y_pred = np.mean(y_pred, axis = 0)\n",
    "\n",
    "score = pearsonr(y_pred, y)[0]\n",
    "print(f\"Overall score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f_0      0.001637\n",
      "f_1     -0.012578\n",
      "f_2      0.008515\n",
      "f_3     -0.020037\n",
      "f_4     -0.005416\n",
      "           ...   \n",
      "f_295   -0.027737\n",
      "f_296   -0.001518\n",
      "f_297   -0.007058\n",
      "f_298   -0.004217\n",
      "f_299   -0.004319\n",
      "Length: 300, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "tmp = pd.read_csv(\"C:\\\\Users\\\\Mengxiao.Wu\\\\Desktop\\\\train\\\\train.csv\")\n",
    "X = tmp.drop(['row_id', 'time_id', 'investment_id', 'target'], axis=1)\n",
    "\n",
    "X = np.mean(X, axis=0)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
